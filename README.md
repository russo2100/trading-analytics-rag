# EventHorizon RAG/KAG System

Production-grade RAG (Retrieval-Augmented Generation) system for natural gas trading analytics. Built as a portfolio project demonstrating best practices for LLMOps: multi-source ingestion, hybrid retrieval, evaluation-driven development, agentic workflows.

## ğŸ¯ Project Goals

1. **Multi-source knowledge base**: Ingest trading bot logs (JSONL), market reports (PDF), news (TXT/MD)
2. **Hybrid retrieval**: BM25 + dense embeddings + reranking for high precision/recall
3. **Production-ready**: FastAPI service, Docker compose, monitoring, security (anti-injection)
4. **Measurable quality**: Gold Q/A dataset, automated evaluation (Ragas-like metrics)
5. **Portfolio value**: Clean code, reproducible, documented design decisions

## ğŸ“Š Current Status (Phase 1: Foundation)

| Component | Status | Details |
|-----------|--------|---------|
| Data Schema | âœ… Done | SQLite: `trading_events`, `sessions`, `trades` |
| Ingestion | âœ… Done | `ingest_trading_logs.py` (v1/v2 support) |
| Data Quality | âœ… Done | 1041 events, 26 trades, 0 NULL/duplicates |
| Vector Store | ğŸš§ In Progress | Weaviate/FAISS integration |
| Retrieval | â³ Planned | Hybrid BM25 + dense |
| Generation | â³ Planned | LangChain + OpenRouter |
| Evaluation | â³ Planned | Gold Q/A + metrics |

## ğŸ—ï¸ Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Data â”‚ Multi-format loaders (JSONL, PDF, TXT)
â”‚ Ingestion â”‚ Normalization, deduplication, validation
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: â”‚ SQLite (metadata + FTS5)
â”‚ Storage â”‚ Vector DB (embeddings)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: â”‚ Hybrid search (BM25 + dense)
â”‚ Retrieval â”‚ Query routing, reranking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4: â”‚ LLM integration (OpenRouter/local)
â”‚ Generation â”‚ Prompt engineering, citation tracking
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 5: â”‚ Task planning, multi-step reasoning
â”‚ Agents â”‚ Tool calling (SQL, calculations)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

text

## ğŸ“ Project Structure

eventhorizon-rag/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ layer1_ingestion/ # Data loaders, normalizers
â”‚ â”œâ”€â”€ layer2_storage/ # DB clients (SQLite, Weaviate)
â”‚ â”œâ”€â”€ layer3_retrieval/ # Search, reranking
â”‚ â”œâ”€â”€ layer4_generation/ # LLM wrappers
â”‚ â””â”€â”€ layer5_agents/ # Agentic workflows
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ ingest_trading_logs.py # Trading logs â†’ SQLite
â”‚ â”œâ”€â”€ init_database.py # Schema initialization
â”‚ â”œâ”€â”€ check_database.py # Data quality checks
â”‚ â””â”€â”€ run_eval.py # Evaluation pipeline
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Source files (gitignored)
â”‚ â”œâ”€â”€ eval/ # Gold Q/A dataset
â”‚ â””â”€â”€ vector_index/ # Vector DB index
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ architecture.md # Design decisions
â”‚ â”œâ”€â”€ eval_baseline.md # Baseline metrics
â”‚ â””â”€â”€ roadmap.md # Development plan
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ unit/ # Unit tests
â”‚ â””â”€â”€ integration/ # Integration tests
â”œâ”€â”€ .env.example # Environment template
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ docker-compose.yml # Local dev stack
â””â”€â”€ README.md # This file

text

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- SQLite 3.35+ (for FTS5)
- (Optional) Docker for Weaviate

### Installation

```bash
# Clone repo
git clone https://github.com/russo2100/eventhorizon-rag.git
cd eventhorizon-rag

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env
# Edit .env: add API keys (OpenRouter, etc.)
Initialize Database
bash
# Create tables
python scripts/init_database.py

# Import sample data (if you have trading logs)
python scripts/ingest_trading_logs.py --source data/raw/logs.jsonl

# Verify
python scripts/check_database.py
ğŸ“ˆ Data Schema
trading_events (bot decisions)
event_id (PK): Deterministic (YYYYMMDD_cycle_unix)

session_id: Daily session (YYYYMMDD)

timestamp, cycle, price, rsi, lots, pnl_pct

action (BUY/SELL/NOOP), reason, ai_signal, ai_confidence

sessions (daily aggregates)
session_id (PK)

total_cycles, total_trades, final_lots, final_pnl_pct

trades (non-NOOP actions)
trade_id, event_id (FK), action, lots_changed, price_usd

ğŸ§ª Testing
bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src tests/

# Run specific suite
pytest tests/unit/test_normalizers.py
ğŸ“š Documentation
Architecture & Design

RAG/KAG Roadmap

Evaluation Methodology

ğŸ›¡ï¸ Security
Prompt injection protection: Retrieved context treated as untrusted data

Secrets management: .env file (gitignored), no hardcoded keys

Input validation: Pydantic models for all external data

ğŸ“ License
MIT License - see LICENSE file

ğŸ‘¤ Author
Ğ ÑƒÑĞ»Ğ°Ğ½ Ğ›Ğ°Ñ‚Ñ‹Ğ¿Ğ¾Ğ² (@russo2100)
AI/ML Engineer | RAG/KAG Systems | LangChain/LlamaIndex

Status: Active development (Phase 1: Foundation âœ… | Phase 2: Retrieval ğŸš§)

Last updated: 2026-02-01

